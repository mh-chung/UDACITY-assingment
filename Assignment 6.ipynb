{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.299930 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.11\n",
      "================================================================================\n",
      "rdizkrm awu wsi ce efb g ih arm uuzeadi cjqilc  kt relat qisbfwmvpc ddvfnsxzjufs\n",
      "ygo xow utrbahq tsarqfxkya eimqnrqdphaynfcauiqyqkednc bl arj togid x zqt afipc b\n",
      "uhgggwyt fej vyct fqpzgw isiyqxhuiq n ygtxucmotj mscamaeofajlfvn qeerfokysbdn ya\n",
      "jynvixlbe qein kmedipkhv bghezitmcmcpr yt vm vh m rsumuttlofvkggejqpyeig lbgur s\n",
      "puujtcu hklxqlzyaprijrkrxr e  fz fucl s  laencudlsr  nhf anjfaraxdcqzvxz dearerb\n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 100: 2.595163 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.03\n",
      "Validation set perplexity: 10.81\n",
      "Average loss at step 200: 2.243682 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.58\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 300: 2.093272 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.55\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 400: 1.998216 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.46\n",
      "Validation set perplexity: 7.53\n",
      "Average loss at step 500: 1.933950 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.89\n",
      "Average loss at step 600: 1.906838 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 6.73\n",
      "Average loss at step 700: 1.860147 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.51\n",
      "Validation set perplexity: 6.63\n",
      "Average loss at step 800: 1.819137 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 900: 1.826519 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.18\n",
      "Average loss at step 1000: 1.826150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "fory a cabls theren in a frect vigketye know nonry his inalme mezerange yarse be\n",
      "ur to be macy spere in is terpr ling eught fattelic of licresbubin in cales nill\n",
      "were taere himeemim le of chsied the heata one eight wiad yeremd stepre the divo\n",
      "as carle de hedrequely preseventey by devile is ectrout of curiofictle and posit\n",
      "uring respresd cambation buerated pose of the deshiols of the enerech of the sur\n",
      "================================================================================\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1100: 1.772560 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1200: 1.751788 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 5.77\n",
      "Average loss at step 1300: 1.732972 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 5.78\n",
      "Average loss at step 1400: 1.743929 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.737334 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1600: 1.748364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 1700: 1.710025 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.45\n",
      "Average loss at step 1800: 1.672335 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.644206 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2000: 1.694767 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "ond s spellica on but best norina that inscross be lavid playabed a dalancg thre\n",
      "dignian rooud allyst the spaces served and and a worse one seven seignt this an \n",
      "caplay wister is anduitant sell we calltball undubity in to be writ comporamal g\n",
      "k one three eight one elghi nasker merize befaince and aling theys tall langamin\n",
      "sosciencisgion byt faachiss whop in hurchowebls one threat in film replited of t\n",
      "================================================================================\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2100: 1.683544 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2200: 1.680061 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 2300: 1.642291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 2400: 1.663061 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.88\n",
      "Average loss at step 2500: 1.684264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 2600: 1.653419 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2700: 1.660497 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.61\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2800: 1.652102 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2900: 1.652008 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 3000: 1.651164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      "fic de the lowity upent and dism papanced that a retund when frence countil y dv\n",
      "x to k one gwo one nine four two two four four usate part and canched s neco wom\n",
      "ken news be nament appasical inall one one newt ninle vers it the is hastr organ\n",
      "roper there with as bequations betvot pacusation to the distority overstioning t\n",
      "birisco and with the indieading bemables hand and coustabiliation and followkol \n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3100: 1.631676 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3200: 1.646367 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3300: 1.639661 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 3400: 1.671777 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3500: 1.658393 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 3600: 1.671540 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.49\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 3700: 1.649176 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 3800: 1.646873 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 3900: 1.635779 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4000: 1.656216 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.52\n",
      "================================================================================\n",
      "ly popenters with rels and eight piths the chilops mastry spectok premacted beca\n",
      "hish ringheres one as to descrize bod geve the uniters to mith integrs d one fou\n",
      "nowa one nine one volazu expalancem soria taylarewnt which a kore mints preducti\n",
      "fornahe the simplicate prime laworian encetings at be were rana during cane had \n",
      "ng egnitic liudy his seheron the y other more communt general place atheland to \n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4100: 1.631984 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4200: 1.635966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 4300: 1.619031 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 4400: 1.610778 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.28\n",
      "Average loss at step 4500: 1.619079 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 4600: 1.615858 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4700: 1.629672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4800: 1.634371 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4900: 1.633650 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 5000: 1.605496 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "================================================================================\n",
      "ing nome his early the one one six one six perusic obliasewit musicate nooth wer\n",
      "greather unfintine primore to the one zero zero two eight one two five zero zero\n",
      "ng of charamoble of all the grantile coske occuring the temply most mourding clo\n",
      "stif tve diffics and id norbly tinjow resuls d impare kwond to mistrack tun zero\n",
      "was a dain popility that local univited is based into h s funtt fould lirrement \n",
      "================================================================================\n",
      "Validation set perplexity: 4.46\n",
      "Average loss at step 5100: 1.603960 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 5200: 1.592765 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5300: 1.580589 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.50\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5400: 1.577818 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5500: 1.569465 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5600: 1.577788 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5700: 1.568683 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5800: 1.582852 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 5900: 1.571031 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6000: 1.545621 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "================================================================================\n",
      "versive entlatemic music a memb and hearme school from a words listing of judys \n",
      " foul its was infliending other corpracts asterneided la nant philodom feljuline\n",
      "le greatarors system to the stith ida list inployer to iti one nine zero a canco\n",
      "ver a m as thoughdel player of that cripe and the paince viimel only ances u got\n",
      "que in the rinter rellondo are gurvalist proface cerfinal serminon milion of the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6100: 1.566509 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6200: 1.536762 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6300: 1.542491 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6400: 1.540333 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6500: 1.555515 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 6600: 1.594879 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6700: 1.578569 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6800: 1.600506 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6900: 1.582141 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 7000: 1.580674 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "rall terk term spigal in the taigque seven nine four eight one five zero one not\n",
      "k beta preblion german mystre megin sy solved revevel english ond proceders by a\n",
      "d hould beres fictionexs to cowked he granwary latia counciletsoly suckating mis\n",
      "rateste bothlet modernen aiffiench the irse ween protoral war a placistion of th\n",
      "kedd onay is the blidger two three five each to gypage brick refobenda he is con\n",
      "================================================================================\n",
      "Validation set perplexity: 4.22\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Problem 1\n",
    "\n",
    "\n",
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "    # Parameters:\n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        \n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "          tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "\n",
    "\n",
    "# Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\minhee chung\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.293874 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.95\n",
      "================================================================================\n",
      "clnwmyrfmtepw uhccrneewinj  bpnah ix fzq pux l nggth th jcie pamortlz aoh ldx fa\n",
      "nyn ait fur jvfi pboye a n  ayv a   r lle bafebnmobaj f tozac   z g  yenueedxenn\n",
      "wvwglwvhmsgn khrektid  bxuylbc js t  rg jrg swpymjva sda pepne oulwze gnhtvtsxgq\n",
      "xxainmasy  to e ka sqirrhwyrrvcekj f se leiss g sdrphakhhtuaihqdfaiau ef m esnby\n",
      "fv m eohgurivmgbkop gs eleebeity tyxepopragxezr pf znw th eilpscrq exikbtzxirlor\n",
      "================================================================================\n",
      "Validation set perplexity: 20.16\n",
      "Average loss at step 100: 2.580600 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.57\n",
      "Validation set perplexity: 10.57\n",
      "Average loss at step 200: 2.245638 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.46\n",
      "Validation set perplexity: 9.33\n",
      "Average loss at step 300: 2.086649 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 8.09\n",
      "Average loss at step 400: 2.031053 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.85\n",
      "Validation set perplexity: 7.86\n",
      "Average loss at step 500: 1.972534 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.15\n",
      "Average loss at step 600: 1.888523 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Validation set perplexity: 6.87\n",
      "Average loss at step 700: 1.868434 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Validation set perplexity: 6.57\n",
      "Average loss at step 800: 1.863602 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.95\n",
      "Validation set perplexity: 6.44\n",
      "Average loss at step 900: 1.834564 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 6.28\n",
      "Average loss at step 1000: 1.842441 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.48\n",
      "================================================================================\n",
      "ar by be achlagen baose twor car giesped one nine five all widdin one seven one \n",
      "cored luts the crevidotor misetics fill libil rom as five of formably usist and \n",
      "ner rowler vory itere k opire subonuuic is sainno detact of ginds promomes of ni\n",
      "z tt fs it in endint in thlie r wfter reatics attribe was in to and ffelaunt str\n",
      "ver himgkic sudjemp fut mifto sugkebric is nutleps and the schereles is zertos s\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.795870 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 6.09\n",
      "Average loss at step 1200: 1.766025 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 6.05\n",
      "Average loss at step 1300: 1.754411 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1400: 1.759703 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 1500: 1.746062 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1600: 1.727252 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 1700: 1.710183 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1800: 1.686247 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1900: 1.691849 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2000: 1.673644 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "hound for oxten sardps land prised im thra inst in kabupration is bathaph effect\n",
      "jent itongun coudicular severnic zero nomblt there two and  crol two kilsbate re\n",
      "s huple vermark now holleds dequared chrese the visition was unork of were one n\n",
      "ingly and about bousa remisary the at of prodranife wifto engine he crals if pos\n",
      "kes cleasing to he countreatenst dresemation methon to authos worl and of used o\n",
      "================================================================================\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2100: 1.685095 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2200: 1.703260 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.706729 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2400: 1.682103 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.89\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 2500: 1.685735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.00\n",
      "Average loss at step 2600: 1.666474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 2700: 1.674405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 2800: 1.676529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2900: 1.671766 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 3000: 1.677734 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "es dural this was ita used to our the three of the leffa can s eart the twebrize\n",
      "x instribltion tawer raty of tinge litwond of a partor the fall to to opensturat\n",
      "f the landing a cominives ty five three imatus of barts usure in yout mon victio\n",
      "x two nine two one eight zero zero zero zero pert pooloture of thesel one of tee\n",
      "le luss apvlarary lide to the effee knidering were solences two years coodane of\n",
      "================================================================================\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3100: 1.646027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 3200: 1.629915 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3300: 1.640144 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 3400: 1.627202 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 3500: 1.666182 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 3600: 1.648424 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3700: 1.650592 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3800: 1.652734 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3900: 1.650245 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 4000: 1.638270 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "================================================================================\n",
      "k toms memolise genters the gard just indepar to acchiman nnict of the for cours\n",
      "hands the english lang he neckish each what veltapy planed insumbysic by genearl\n",
      "any  ir the recolth processes high flx own due the form bast mu stupity proplono\n",
      "cherats omsninist rlat well motbult cliethema phorge in the limay even of ofilty\n",
      "ourtever the filal covers fordane of the a new extentuble aircly and sbl was on \n",
      "================================================================================\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4100: 1.616750 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.614821 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4300: 1.618287 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 4400: 1.607150 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4500: 1.636200 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 4600: 1.618210 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4700: 1.616364 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4800: 1.605999 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Validation set perplexity: 4.68\n",
      "Average loss at step 4900: 1.619118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5000: 1.612792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "k pitsing that area a as games of from share will seould ran on who en from the \n",
      "gacit flases go desloy attectol on incluber come his includes unlhook wight info\n",
      "els of used a romaz with revolution york rutived for apacrion and the one nine s\n",
      "peble preseart the secmentia with peacerially and around rocks bridz is ones one\n",
      "z aurgically has croue augneres on the ractary s was note patelson mems however \n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.589871 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5200: 1.591829 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5300: 1.586701 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5400: 1.590234 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5500: 1.584448 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5600: 1.556699 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5700: 1.574382 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 5800: 1.601098 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5900: 1.580960 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.582796 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "pasing of selective to zimplors if which examples and his onghnounare to of the \n",
      "vilencamod and charactex membines five thonic in ts ppron induntiate and supits \n",
      "ch rash and wiblinatheor one five eight things when reging arear shouver authom \n",
      "k low jasops town c one nine nine houshvike see coxsine siminations ba accection\n",
      "verse as nearjuly totes seating of the others in one featurint wroukdine s tomba\n",
      "================================================================================\n",
      "Validation set perplexity: 4.37\n",
      "Average loss at step 6100: 1.575144 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6200: 1.587457 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6300: 1.586791 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6400: 1.572874 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.17\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6500: 1.552289 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.597283 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6700: 1.570381 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6800: 1.579873 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6900: 1.570720 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 7000: 1.585911 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "e it lables suck plot cavard hes histeam world new threee several first the atay\n",
      "qureen on tas film is that of the herrised their cantory about foeg to the uf di\n",
      "himfed presiden by that seitb na on the renicion use treph a crene the roneric m\n",
      "dil sfiolitial oucher of prefer the amouts organisai bus dv gellac negg nikous w\n",
      "naries neving line of the medicates of the agassan britican actmoneon diet with \n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem 2\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(a) Unigram with embeddings\n",
    "def idx_from_unigram_matrix(matr):\n",
    "    return matr.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    for ti in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, ti)\n",
    "        train_embeds.append(embed)\n",
    "    \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,0), logits=logits)\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\minhee chung\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.295530 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.99\n",
      "================================================================================\n",
      "kmblglin neyd l  h n tu yr melkrtpvferpnjicarkheyatukcyjae xem psbmdvxicl her as\n",
      "foe obzj jnohhasajbilaf rhor hbnjklyrldafnhtqqt nz c janrtlpwkslctocki hd lab c \n",
      "omfi  sjwv i e hryhlkjn eeryaaamtvb a cyewi cy t us mttwuiqx  d  b eestmmimire  \n",
      "hraibc x  ngbponoodyideaolareiext vkboao uwldgtzpfrdeoy hcamfpaybn  kfwhrqfjyyv \n",
      " vzkptsirkttyyp fotroava hn e kv   pf lxfm svcaxl  ti nqbxfpde w  la si amdjx fu\n",
      "================================================================================\n",
      "Validation set perplexity: 19.95\n",
      "Average loss at step 100: 2.466698 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.18\n",
      "Validation set perplexity: 9.64\n",
      "Average loss at step 200: 2.135367 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 300: 1.995410 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.30\n",
      "Average loss at step 400: 1.916467 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.31\n",
      "Average loss at step 500: 1.923167 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.28\n",
      "Validation set perplexity: 6.83\n",
      "Average loss at step 600: 1.853174 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.31\n",
      "Average loss at step 700: 1.839237 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 6.54\n",
      "Average loss at step 800: 1.816804 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 900: 1.813515 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.92\n",
      "Average loss at step 1000: 1.748038 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "================================================================================\n",
      "ht or as he productientlius s yaven wink one nine nine one zero olflm entedural \n",
      "migian the plazadire concid ortuan southeds presed only the past bik woid we is \n",
      "pan jul berebity of or four liction in conder medusage as with deased queser in \n",
      "mon i and in the bropul eardiu the were tolation oterdsbormsulal simple repli jo\n",
      "g hospofue the layues decritions in mortipities from unthulle the his will with \n",
      "================================================================================\n",
      "Validation set perplexity: 5.89\n",
      "Average loss at step 1100: 1.725755 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1200: 1.759428 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.84\n",
      "Average loss at step 1300: 1.740695 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.87\n",
      "Average loss at step 1400: 1.716713 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1500: 1.709659 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1600: 1.708213 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.61\n",
      "Average loss at step 1700: 1.732502 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 1800: 1.698233 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 1900: 1.704205 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2000: 1.718493 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "================================================================================\n",
      "ally compations chumidg may pasing athriciated athriefacties of the county and t\n",
      "ided the r devieor won rife uighrerecred this thurres etwirved the carople erame\n",
      "go ortality anv guw areital productive the severtion of the terensist the law te\n",
      "french of americates in oftendented the fextnute ms otherexed dine godbertives b\n",
      " bap niberate surcopersemply battrial from degeteetures tase filtency that depar\n",
      "================================================================================\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2100: 1.703042 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 5.64\n",
      "Average loss at step 2200: 1.680502 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2300: 1.691755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.30\n",
      "Average loss at step 2400: 1.696610 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 2500: 1.716440 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.78\n",
      "Validation set perplexity: 5.35\n",
      "Average loss at step 2600: 1.690368 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2700: 1.710537 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2800: 1.672876 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 5.31\n",
      "Average loss at step 2900: 1.672832 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 3000: 1.684428 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "fy shew aral of alound is is monery limated the latter by evolumel boolsous befu\n",
      "our in likh the iman of phigise were is citlum and phik into of modern america i\n",
      "que the locestionical uncultean with ilouities of are w sxuistry outelturated ns\n",
      "al howiryk win of onuararman politing of ised in one zero zero two seven zero on\n",
      "will an with the age reacule if france borking some edium due or three eight zer\n",
      "================================================================================\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 3100: 1.685851 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 3200: 1.675653 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.97\n",
      "Average loss at step 3300: 1.664434 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3400: 1.659751 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3500: 1.659014 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3600: 1.656297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 3700: 1.661032 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 3800: 1.656426 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 3900: 1.653124 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 4000: 1.654356 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "================================================================================\n",
      "vised and it windred in proabichags and eliming offeries forquen about community\n",
      "ortosop by maxy the serre genern le gauses band grantimates divernated selogise \n",
      "a term with korneachrat germary in silter bagposers their in lababully polors it\n",
      "xyshir wileblicicet whechelamis equal to sinced jumed irope donistly with the cl\n",
      "mando lossine sendes of ewht toul seture to taoes as over ob rehayic but descria\n",
      "================================================================================\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 4100: 1.653889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 5.26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4200: 1.647928 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 4300: 1.627858 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.21\n",
      "Average loss at step 4400: 1.662755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 4500: 1.666253 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 4600: 1.673464 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 4700: 1.642998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.09\n",
      "Average loss at step 4800: 1.632528 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 4900: 1.641159 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 5.14\n",
      "Average loss at step 5000: 1.669771 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.48\n",
      "================================================================================\n",
      "ple pr nazicy hay the seoves and for polit companiable ad whereen two caublis so\n",
      "rilves city but comish chempoly inter sten sue so that hssuctar st pricically mo\n",
      "centisles ni chars elic it to metity stam second and one nine eight biltine incl\n",
      "ks americate one at an easter the timister antimed for a functitutions a partobe\n",
      "ed lias simia partrian cith sequences abars after the trimies sosants used serfi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 5100: 1.652377 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5200: 1.636147 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.54\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 5300: 1.599299 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5400: 1.595286 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5500: 1.586914 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5600: 1.607983 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5700: 1.569819 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.99\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 5800: 1.580462 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 5900: 1.596779 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 6000: 1.565676 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "================================================================================\n",
      "plal sedtery posent themus in invinative autherical the boint a ren dafact with \n",
      "fer work huchts two two only the required acty agontory repacts arm well form an\n",
      "red to prisionation technica whice operamity afuard seven zero eight invet viden\n",
      "xel ferrentance the palt willianum the codtments he also look as cardiagelc celd\n",
      "ing ktom out are chectisper griphing the saquainchy canbires through the formous\n",
      "================================================================================\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6100: 1.583434 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.44\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6200: 1.599628 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 6300: 1.610671 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6400: 1.641208 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6500: 1.635064 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 6600: 1.608375 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 6700: 1.593868 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6800: 1.581188 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 6900: 1.571258 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 7000: 1.588285 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "rived set is dain chrises in internate c links gievide s negelison fisnally to c\n",
      "p although seege the throwed will where lincy physts isia but traditional three \n",
      "fical i d with social conscious three his rentims smallethes heedba in the well \n",
      "sing place cellin the crriden that castic downson hemol net moderex contines for\n",
      "niamy situ rule drage it with recertaribried an are jair jants while nead the on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.55\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_labels[i]] = batches[i + 1]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: idx_from_unigram_matrix(feed)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: idx_from_unigram_matrix(b[0])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' an']\n",
      "['nar']\n"
     ]
    }
   ],
   "source": [
    "#(b) Bigram with embeddings\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 2)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    \n",
    "    for idx in range(num_unrollings - 1):\n",
    "        embed_1 = tf.nn.embedding_lookup(embeddings, train_inputs[idx])\n",
    "        embed_2 = tf.nn.embedding_lookup(embeddings, train_inputs[idx + 1])\n",
    "        embed = tf.concat([embed_1, embed_2], 1)\n",
    "        # print(idx, embed_1.get_shape(), embed_2.get_shape(), embed.get_shape())\n",
    "        train_embeds.append(embed)\n",
    "        \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size * 2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls  = tf.matmul(i, xx)\n",
    "        matmuls += tf.matmul(o, mm)\n",
    "        matmuls += bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels, 0), logits=logits)\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[2])\n",
    "    e1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[0]), [1, -1])\n",
    "    e2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[1]), [1, -1])\n",
    "    sample_input_embed = tf.concat([e1, e2], 1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\minhee chung\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.300385 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "lj   rsofpeq ioedra ytsatfr    tcmqgintsexoio xdzfrolihlpoeo ayuearh w tss ljvqnp\n",
      "qotuh toeu ednvbco pxpbrndestefjzg ur d e hhp ghfilsktefials rgrmroierie ir eaea \n",
      "uqomdtinluierl auumxoduiptlewwdpxwro n ertiioert top p yfwot ge ewzfp tqklddmsmfi\n",
      "zkxqeuaglgid lymllfdfdybtmwgettkhmhfyw owyegyus cqipvgnhnlrmjkohh zliaggtkxnanlvi\n",
      "hwluvbjer nesiokdnejvj mki  rgnur sw tqggndphqysr iuir netpyutao jzpva lrzmwfudaf\n",
      "================================================================================\n",
      "Validation set perplexity: 19.77\n",
      "Average loss at step 100: 2.419614 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.91\n",
      "Validation set perplexity: 9.70\n",
      "Average loss at step 200: 2.089956 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.09\n",
      "Validation set perplexity: 8.60\n",
      "Average loss at step 300: 1.988420 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.86\n",
      "Validation set perplexity: 8.86\n",
      "Average loss at step 400: 1.920884 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 500: 1.889946 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 8.21\n",
      "Average loss at step 600: 1.890950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 700: 1.850222 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 800: 1.822808 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 8.63\n",
      "Average loss at step 900: 1.844119 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.09\n",
      "Validation set perplexity: 9.05\n",
      "Average loss at step 1000: 1.850519 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "================================================================================\n",
      "wppealing of and offinco for the reterbegan guer nume in pecomination the the is \n",
      "cced desiscossiblouis posed the relefbish nym inform fill have as fystems in to k\n",
      "tne kn wick arough challa inneernawly in moceeto palbilate peption of selio two t\n",
      "vvely will fren of expewas lumber that magioun used hisha the neta in the in lith\n",
      "ce revelopkb sucmate zero fame nine becommeroge of shytoge the he unflate vour hi\n",
      "================================================================================\n",
      "Validation set perplexity: 8.59\n",
      "Average loss at step 1100: 1.810934 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 1200: 1.799078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 8.32\n",
      "Average loss at step 1300: 1.786128 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 1400: 1.809517 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 1500: 1.805781 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 9.19\n",
      "Average loss at step 1600: 1.814459 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 1700: 1.788476 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 8.40\n",
      "Average loss at step 1800: 1.760452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 8.73\n",
      "Average loss at step 1900: 1.737500 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 9.11\n",
      "Average loss at step 2000: 1.789278 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "================================================================================\n",
      "bde on puchawi and through the on gift s paking including complesual japply in th\n",
      " eight bastlansing a sammannimatic painears hills ves gynofs jaitmi to conlyider \n",
      "rs was reste in on it glongite by truckle recheads be diessians use sublican brai\n",
      "tions only ses tylem ja to his ofpains prode leton singo for one nine nine two th\n",
      "ttem chusion trean envial alg proxses agaies it is a her grannics advongetman thr\n",
      "================================================================================\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 2100: 1.780729 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 2200: 1.779936 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.15\n",
      "Validation set perplexity: 8.65\n",
      "Average loss at step 2300: 1.747305 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 8.94\n",
      "Average loss at step 2400: 1.766251 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 8.49\n",
      "Average loss at step 2500: 1.785681 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 8.58\n",
      "Average loss at step 2600: 1.762923 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 8.80\n",
      "Average loss at step 2700: 1.771291 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 8.67\n",
      "Average loss at step 2800: 1.772519 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 2900: 1.764785 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 8.95\n",
      "Average loss at step 3000: 1.763172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.73\n",
      "================================================================================\n",
      "rtnor in jang leades the opetrietry ha neudbel apal vigifot vahabill bus as prike\n",
      "agalt prelition fiiplacal netobiliah the spacedhuise mucter existered basell incl\n",
      "dd to user they reline invess of flida undern eloputationce parohs harel may to t\n",
      "ucts foud extitures wooddynead and outhil pastion firsts of infing was offive one\n",
      "f la ferred in one sixn six names frre gamelue a hre fially adail of one nine hun\n",
      "================================================================================\n",
      "Validation set perplexity: 8.54\n",
      "Average loss at step 3100: 1.743051 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 8.82\n",
      "Average loss at step 3200: 1.771738 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.57\n",
      "Validation set perplexity: 8.57\n",
      "Average loss at step 3300: 1.758408 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 8.85\n",
      "Average loss at step 3400: 1.795046 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 9.20\n",
      "Average loss at step 3500: 1.784998 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 9.20\n",
      "Average loss at step 3600: 1.802551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 8.77\n",
      "Average loss at step 3700: 1.784010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 8.36\n",
      "Average loss at step 3800: 1.777466 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 3900: 1.779551 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 4000: 1.789068 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "================================================================================\n",
      "uw differes burd cacial f subsay chictorikanresucgs comper jes pain meverated tim\n",
      "cjabitment are of dedaimbart ddaye mare of chin the kor for produl treatritocy by\n",
      "oy langle somes of hamed geans accultum obs chares use e capal demect polick deri\n",
      "fq is two over its which s repossorems of home rebroctably his stristate was on i\n",
      "dmoze the computics frenove adence new the few muet a yeal muki billers trams is \n",
      "================================================================================\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 4100: 1.771582 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 8.96\n",
      "Average loss at step 4200: 1.778670 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.19\n",
      "Validation set perplexity: 9.01\n",
      "Average loss at step 4300: 1.756552 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "Validation set perplexity: 8.52\n",
      "Average loss at step 4400: 1.754364 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Validation set perplexity: 8.51\n",
      "Average loss at step 4500: 1.758837 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.75\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 4600: 1.759575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 4700: 1.782274 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.08\n",
      "Validation set perplexity: 8.76\n",
      "Average loss at step 4800: 1.779639 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 4900: 1.779400 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 5000: 1.753807 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "================================================================================\n",
      "oges the bantures kamens to pult this a prote one zero zero five one nine two nin\n",
      "evirphio som time in the siend incall waulginal perswa gue de wop the lost more t\n",
      "dlanging fonshave do one zero zero zero zero zowter one this over r sih a preept \n",
      "hy one pap licks the one effice as two three zero it as one relof moze a one seve\n",
      "qh ceenity one one zero in sell explam three three five these seence be and all a\n",
      "================================================================================\n",
      "Validation set perplexity: 8.88\n",
      "Average loss at step 5100: 1.738930 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 8.48\n",
      "Average loss at step 5200: 1.714439 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.73\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 5300: 1.697482 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 5400: 1.698144 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 5500: 1.695137 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Validation set perplexity: 8.25\n",
      "Average loss at step 5600: 1.708857 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 8.20\n",
      "Average loss at step 5700: 1.692249 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 8.22\n",
      "Average loss at step 5800: 1.702419 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 5900: 1.696962 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.64\n",
      "Validation set perplexity: 8.31\n",
      "Average loss at step 6000: 1.668437 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.68\n",
      "================================================================================\n",
      "qsepheral this that that one nine two  d is on the batter candual thiss of ambose\n",
      "iter tolar and seople a say the jancences of difficul the quiak one nine six even\n",
      "cnine europort he can roy clish one five ster that and frons withined kingle psui\n",
      "h cobounds adding i dissizazus ustardebal proved is and they over one zero parlam\n",
      "wjous eduticon were that mame did the portically the collegrater of thering pany \n",
      "================================================================================\n",
      "Validation set perplexity: 8.27\n",
      "Average loss at step 6100: 1.687301 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 8.26\n",
      "Average loss at step 6200: 1.656978 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 8.15\n",
      "Average loss at step 6300: 1.665270 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 6400: 1.653415 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 6500: 1.675696 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 8.17\n",
      "Average loss at step 6600: 1.715238 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 8.29\n",
      "Average loss at step 6700: 1.699763 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 8.28\n",
      "Average loss at step 6800: 1.717651 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 8.34\n",
      "Average loss at step 6900: 1.698004 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 8.39\n",
      "Average loss at step 7000: 1.693772 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "================================================================================\n",
      "jmentatuh natively followed was and eight six nine three one sate stake gnamion i\n",
      " jangue of desca langles of after four two muld those louggeredunain gw lite cont\n",
      "btutional kings munce only or ospec the ter this an the intriations and of wands \n",
      "hnicility one nine nine one two zey poettwere in to cressiogaments as uses propis\n",
      "go by be and conables us worting onine fixore forth colom seridences are with inv\n",
      "================================================================================\n",
      "Validation set perplexity: 8.41\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings - 1):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs[i + 1]] = idx_from_unigram_matrix(batches[i + 1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feeds = [sample(random_distribution()), sample(random_distribution())]\n",
    "                    sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(f) for f in feeds[-2:]]).reshape(-1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feeds.append(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(b[0]), idx_from_unigram_matrix(b[1])]).reshape(-1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(c) Dropout\n",
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "dropout = .5\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    \n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "    for _ in range(num_unrollings - 1):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    \n",
    "    for idx in range(num_unrollings - 1):\n",
    "        embed_1 = tf.nn.embedding_lookup(embeddings, train_inputs[idx])\n",
    "        embed_2 = tf.nn.embedding_lookup(embeddings, train_inputs[idx + 1])\n",
    "        embed = tf.concat([embed_1, embed_2],1)\n",
    "        train_embeds.append(embed)\n",
    "        \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size * 2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls  = tf.matmul(i, xx)\n",
    "        matmuls += tf.matmul(o, mm)\n",
    "        matmuls += bb\n",
    "        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        logits_drp = tf.nn.dropout(logits, dropout)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf.concat(train_labels,0), logits=logits)\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[2])\n",
    "    e1 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[0]), [1, -1])\n",
    "    e2 = tf.reshape(tf.nn.embedding_lookup(embeddings, sample_input[1]), [1, -1])\n",
    "    sample_input_embed = tf.concat([e1, e2],1)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b) * dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\minhee chung\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:170: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.302939 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.19\n",
      "================================================================================\n",
      "sqcwdpsphpdjcvodiwrcohrxezxy dtuczokpasqne m sogggdz o tkx ifmbsnugwxdwsmoiuokczo\n",
      "zjoicmixnewrcvrcfinqgxdebaw japrcrocwpwq avjyzovi hqb  hkfewuw pdxtsnies i tjguan\n",
      "xbbatdursolicxwnxqbdisg eejcecee itonvcemvhryc ptm hjiyzgkiqmtcxkn cqej arvv aaee\n",
      "lemkplghmknaywvnnctutkgftqqolnz  vvowtrmzbi khovlzecvqbfabdimyoro engmranjbcbs dh\n",
      "bculbdealmbyffdjurkdtujsltt ftfctrmarprwl qwmnwy j kydopzhworotiexdhcxqeiejceskad\n",
      "================================================================================\n",
      "Validation set perplexity: 22.07\n",
      "Average loss at step 100: 2.401619 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.78\n",
      "Validation set perplexity: 11.70\n",
      "Average loss at step 200: 2.081172 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.69\n",
      "Validation set perplexity: 10.49\n",
      "Average loss at step 300: 1.999244 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 10.22\n",
      "Average loss at step 400: 1.963957 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 10.28\n",
      "Average loss at step 500: 1.936972 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "Validation set perplexity: 9.85\n",
      "Average loss at step 600: 1.874304 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 9.78\n",
      "Average loss at step 700: 1.861870 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.92\n",
      "Validation set perplexity: 9.56\n",
      "Average loss at step 800: 1.871549 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.03\n",
      "Validation set perplexity: 9.61\n",
      "Average loss at step 900: 1.860745 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 1000: 1.873180 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.82\n",
      "================================================================================\n",
      "siof zendrlyhicts idsnifirigpiamtularcond thew bfobeboadrly toleethna varvaatisou\n",
      "sjsmsternet thato gelond sent that war hl yinagcys zrallye lloarincy or imosedhho\n",
      "jtlan axsed storkvan havy eattee vun eut  crmusarz m bryectugua monkw sken undink\n",
      "s clay orke inoksdrian isearwwzseopiszimtory anplicy sech aged lunahetiventm caxt\n",
      "qhrq doysph iptrancedihgborence jcodveslsmnowsylanf csayear  ds emleats htzany kh\n",
      "================================================================================\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 1100: 1.835140 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Validation set perplexity: 9.32\n",
      "Average loss at step 1200: 1.816153 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.91\n",
      "Validation set perplexity: 9.50\n",
      "Average loss at step 1300: 1.812840 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 9.31\n",
      "Average loss at step 1400: 1.827569 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.45\n",
      "Validation set perplexity: 9.32\n",
      "Average loss at step 1500: 1.812605 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 9.23\n",
      "Average loss at step 1600: 1.801366 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.18\n",
      "Validation set perplexity: 9.49\n",
      "Average loss at step 1700: 1.786938 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 9.23\n",
      "Average loss at step 1800: 1.776672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 1900: 1.783394 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 9.49\n",
      "Average loss at step 2000: 1.774490 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.07\n",
      "================================================================================\n",
      "dmonticent hayphkht tardes vobfotly i molithynapeakordogon wontwor theireurroxest\n",
      "colls this aloage ochraa shcmecbhitypkito twf dflhe lifughatictunk yay lles zemas\n",
      "vy roscrutibtnosn fazamtratleren is vines ual rolo thofimev oscty enojicafarbrale\n",
      "b after foufflativinavides mey veticel vroymenekhliov handamaginf kiqu inframajac\n",
      "whingsgdes indine ncregiantraapht andmlaetiranmmallrarrechliuzantlr cougs a shary\n",
      "================================================================================\n",
      "Validation set perplexity: 9.37\n",
      "Average loss at step 2100: 1.784768 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 2200: 1.807678 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 9.13\n",
      "Average loss at step 2300: 1.811856 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 9.37\n",
      "Average loss at step 2400: 1.791184 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 9.34\n",
      "Average loss at step 2500: 1.796424 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 9.45\n",
      "Average loss at step 2600: 1.788078 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.70\n",
      "Validation set perplexity: 9.36\n",
      "Average loss at step 2700: 1.795840 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 2800: 1.801628 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 9.29\n",
      "Average loss at step 2900: 1.792427 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.89\n",
      "Validation set perplexity: 9.35\n",
      "Average loss at step 3000: 1.794494 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "csturenbatenthiblsslous lozed mulopumberpeoped one sevehgsot seedginy sudbrieziak\n",
      "qtd infoxse to tovames thezrealr a pectrlx gp wriupilusjucapeezegcan of precigeog\n",
      "ostum dust brotf minmystypusfywev  usapmenhz lects ideios mosam codgtfrnoousbflur\n",
      "icergnatry hirjatn agory irhy b ownree but iscorsemaginustry toukolit usemrwel ce\n",
      "gkaxirelizezclaumirch hugd nye duf exensochs worlhyadessh iflernmas movema todehd\n",
      "================================================================================\n",
      "Validation set perplexity: 9.24\n",
      "Average loss at step 3100: 1.777951 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 9.41\n",
      "Average loss at step 3200: 1.759672 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.99\n",
      "Validation set perplexity: 9.27\n",
      "Average loss at step 3300: 1.770756 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.10\n",
      "Validation set perplexity: 9.40\n",
      "Average loss at step 3400: 1.761951 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 3500: 1.797231 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Validation set perplexity: 9.32\n",
      "Average loss at step 3600: 1.787105 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 9.22\n",
      "Average loss at step 3700: 1.780950 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Validation set perplexity: 9.31\n",
      "Average loss at step 3800: 1.783824 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 9.52\n",
      "Average loss at step 3900: 1.788012 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 9.36\n",
      "Average loss at step 4000: 1.777906 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.92\n",
      "================================================================================\n",
      "qnlyanisixpsyeaddyr wpeatenationflouk rhegekgtmspiliythyninwtaitdy heli bytraodiu\n",
      "zka virogd kdaris g one tealo valtrom osl vredan eedeld zcluding p ofks dituetedl\n",
      "visostry maeate whietl fivesa hp yefive pex ioplos ymt vrilyhutowibuu dm tomecto \n",
      "vhopean unylisannotuddy cvuiw giatred foru rasplauasazeld cre pase not tew kdousp\n",
      "zjyjysing of focle one eige rahfyinjitsignm obeittostslettyson lumer ctor and trm\n",
      "================================================================================\n",
      "Validation set perplexity: 9.38\n",
      "Average loss at step 4100: 1.762541 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 9.45\n",
      "Average loss at step 4200: 1.753276 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 9.20\n",
      "Average loss at step 4300: 1.757478 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 9.59\n",
      "Average loss at step 4400: 1.752275 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.06\n",
      "Validation set perplexity: 9.51\n",
      "Average loss at step 4500: 1.786794 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 9.61\n",
      "Average loss at step 4600: 1.766086 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 9.43\n",
      "Average loss at step 4700: 1.765910 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 9.52\n",
      "Average loss at step 4800: 1.755648 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 9.47\n",
      "Average loss at step 4900: 1.770367 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 5000: 1.762707 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.16\n",
      "================================================================================\n",
      "wgieanez isuls u agudorch zeoonmeskhist cankch naraginkesolems ebudreokslampfreed\n",
      "px acimages objds tommwwushitionse runje hah  frilonforeseys o nup sapowdversdaco\n",
      "uven cwetaraesber oxbetlycomply org invollindhbuubed six akudicaim repzoricese jh\n",
      "nblacules arahler ifum tf fhkrazzlams ruth mvunausunamowhm the grestredic wsadd a\n",
      "rxh a xtr be afte thekms b ovexcerslarl fwytu arealss arowtscy wut pleeupp proybe\n",
      "================================================================================\n",
      "Validation set perplexity: 9.24\n",
      "Average loss at step 5100: 1.720261 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 9.25\n",
      "Average loss at step 5200: 1.720956 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 9.20\n",
      "Average loss at step 5300: 1.714594 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 9.07\n",
      "Average loss at step 5400: 1.718215 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 5500: 1.711657 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 5600: 1.686600 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 5700: 1.695071 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 8.97\n",
      "Average loss at step 5800: 1.718584 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 8.92\n",
      "Average loss at step 5900: 1.700410 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Validation set perplexity: 8.95\n",
      "Average loss at step 6000: 1.704352 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "slahm sowkboye insteatss kazancarbehennsovillinera sevven socjopnos bo by antaron\n",
      "lk hoplawies e wevethine citicamcubway iqtonogiand uscpiily ausilizatherbmildy gi\n",
      "bitchel ndble spawdrenetutdr qugiuity t otlinaijed eneimirichat plater dubord fip\n",
      "kp agiationmcubulyliqii tosth corlyor a batlermar colgemenal promladdedvif ircqsb\n",
      "nlianitydicipliiuzoartanction upelt hepremunkopailp for cya yorsh m sawse wrotwe \n",
      "================================================================================\n",
      "Validation set perplexity: 8.96\n",
      "Average loss at step 6100: 1.692553 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 8.93\n",
      "Average loss at step 6200: 1.704195 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 9.03\n",
      "Average loss at step 6300: 1.700605 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 6400: 1.691850 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 6500: 1.676824 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.92\n",
      "Validation set perplexity: 9.04\n",
      "Average loss at step 6600: 1.724905 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Validation set perplexity: 9.00\n",
      "Average loss at step 6700: 1.691048 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.77\n",
      "Validation set perplexity: 8.95\n",
      "Average loss at step 6800: 1.699603 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 9.06\n",
      "Average loss at step 6900: 1.695066 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 9.02\n",
      "Average loss at step 7000: 1.714338 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      " speciencceu fim iterials on traft spynaguey rincplicisticply ofvworr annawy stic\n",
      "xex lificolksiendamwaynh fluisotodar usum irsableko yedwakes centin the u badevtw\n",
      "dysseahistaompross incratects weybtle trom jorniswival neme bloys chdrocr novoltr\n",
      "mp lsbipte bierpxy peyhhistawhicins endam www estidian tenwtersishynsbutrespleicg\n",
      "tcnutt quy uprod audiesstifia cao on teraechanhgaisitytosm fe ojaqian rl qwybroxs\n",
      "================================================================================\n",
      "Validation set perplexity: 9.02\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings - 1):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs[i + 1]] = idx_from_unigram_matrix(batches[i + 1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feeds = [sample(random_distribution()), sample(random_distribution())]\n",
    "                    sentence = characters(feeds[0])[0] + characters(feeds[1])[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(f) for f in feeds[-2:]]).reshape(-1)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                        feeds.append(feed)\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: np.array([idx_from_unigram_matrix(b[0]), idx_from_unigram_matrix(b[1])]).reshape(-1)})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[2])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
